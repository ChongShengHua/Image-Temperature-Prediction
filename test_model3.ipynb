{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_model3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Mps6yoVvkkei"},"source":["Reference: \n","1. SpinalNet: Deep Neural Network with Gradual Input\n","2. https://github.com/dipuk0506/SpinalNet\n"]},{"cell_type":"markdown","metadata":{"id":"jqA0G_RsklrV"},"source":["# 1.0 Read test image "]},{"cell_type":"code","metadata":{"id":"K6Hn2LUXgJ0V"},"source":["import torchvision\n","from torchvision import transforms\n","import torch.utils.data as data\n","from PIL import ImageFile\n","\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","batch_size_test = 64\n","\n","TRANSFORM_IMG = transforms.Compose([transforms.Resize((240, 320)), transforms.ToTensor()])\n","\n","TEST_DATA_PATH = \"classification/test/\"\n","test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=TRANSFORM_IMG)\n","test_data_loader = data.DataLoader(test_data, batch_size=batch_size_test, shuffle=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V9Ub_P6Alwhg"},"source":["# 2.0 Test Model"]},{"cell_type":"code","metadata":{"id":"Zj930L9GgO2s"},"source":["# classification\n","# first_HL = 8\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchsummary import summary\n","\n","torch.backends.cudnn.enabled = False\n","\n","first_HL = 8\n","\n","class Net(nn.Module):\n","\n","  def __init__(self):\n","      super(Net, self).__init__()\n","      # kernal 5, maxpooling = 2\n","      self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n","      self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","      self.conv2_drop = nn.Dropout2d()\n","      # features shape = 20*57*77\n","      self.fc1 = nn.Linear(int((20*57*77)/2), first_HL)  \n","      self.fc1_1 = nn.Linear(int((20*57*77)/2) + first_HL, first_HL)  \n","      self.fc1_2 = nn.Linear(int((20*57*77)/2) + first_HL, first_HL)  \n","      self.fc1_3 = nn.Linear(int((20*57*77)/2) + first_HL, first_HL)  \n","      self.fc1_4 = nn.Linear(int((20*57*77)/2) + first_HL, first_HL)  \n","      self.fc1_5 = nn.Linear(int((20*57*77)/2) + first_HL, first_HL) \n","      self.fc2 = nn.Linear(first_HL*6, 70)  \n","        \n","\n","  def forward(self, x):\n","    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n","    x = x.view(-1, 20*57*77)\n","    x1 = x[:, 0:int((20*57*77)/2)]\n","    \n","    x1 = F.relu(self.fc1(x1))\n","    x2 = torch.cat([ x[:,int((20*57*77)/2):20*57*77], x1], dim=1)\n","    x2 = F.relu(self.fc1_1(x2))\n","    x3 = torch.cat([ x[:,0:int((20*57*77)/2)], x2], dim=1)\n","    x3 = F.relu(self.fc1_2(x3))\n","    x4 = torch.cat([ x[:,int((20*57*77)/2):20*57*77], x3], dim=1)\n","    x4 = F.relu(self.fc1_3(x4))\n","    x5 = torch.cat([ x[:,0:int((20*57*77)/2)], x4], dim=1)\n","    x5 = F.relu(self.fc1_4(x5))\n","    x6 = torch.cat([ x[:,int((20*57*77)/2):20*57*77], x5], dim=1)\n","    x6 = F.relu(self.fc1_5(x6))\n","\n","    # before fc2  \n","    x = torch.cat([x1, x2], dim=1)\n","    x = torch.cat([x, x3], dim=1)\n","    x = torch.cat([x, x4], dim=1)\n","    x = torch.cat([x, x5], dim=1)\n","    x = torch.cat([x, x6], dim=1)\n","\n","    # fc2\n","    x = self.fc2(x)\n","\n","    return F.log_softmax(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGSk4yDrge0b"},"source":["import os\n","from itertools import chain\n","import shutil\n","\n","folder = 'newClassification/model3'\n","tested_result = pd.DataFrame(columns = ['model', 'epoch', 'rmse'])\n","model = Net()\n","\n","for filename in os.listdir(folder):\n","    print(filename)\n","    model = torch.load(folder + '/' + filename)\n","    model.eval()\n","    predicted = []\n","    actual = []\n","    \n","    for data,target in test_data_loader:\n","        output = model(data)\n","        predicted.append(output.data.max(1)[1].cpu().data.numpy())\n","        actual.append(target.cpu().data.numpy())\n","        \n","    loss_rmse = cal_rmse(list(chain.from_iterable(predicted)), list(chain.from_iterable(actual)))\n","    \n","    name = filename.split('.')[0]\n","    name = name.split('_')\n","    model_name = name[0]\n","    epoch = name[2]\n","    print(model_name)  \n","    print(epoch)  \n","    \n","    tested_result.loc[len(tested_result)] = [model_name, epoch, loss_rmse]\n","    print(tested_result)  \n","    \n","    filename = 'tested_result_' + str(model_name) + '_' +str(epoch)+ '.csv'\n","    tested_result.to_csv(filename, encoding='utf-8', index=False)\n","    shutil.move(os.getcwd() + '\\\\' + filename, os.getcwd() + '\\\\newClassification')\n","\n","    "],"execution_count":null,"outputs":[]}]}